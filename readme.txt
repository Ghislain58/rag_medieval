PrÃ©requis

Windows 10/11 avec WSL2 (Ubuntu)

Docker Desktop configurÃ© avec backend WSL

GPU NVIDIA compatible CUDA + drivers Ã  jour

NVIDIA Container Toolkit installÃ© cÃ´tÃ© WSL (pour --gpus all)

Dans WSL, vÃ©rifier :

nvidia-smi
docker run --rm --gpus all nvidia/cuda:12.2.2-runtime-ubuntu22.04 nvidia-smi

DÃ©marrage rapide

Depuis WSL (Ubuntu) :

cd ~/rag_medieval

# 1. Construire l'image (GPU)
docker build --no-cache -t rag_medieval_app .

# 2. RÃ©seau Docker pour RAG + Ollama
docker network create rag-net || true

# 3. Lancer Ollama (LLM local)
docker run --gpus all -d \
  --name ollama \
  --network rag-net \
  ollama/ollama

# 4. TÃ©lÃ©charger le modÃ¨le Mistral dans Ollama
docker exec -it ollama ollama pull mistral

# 5. Lancer l'application RAG (Streamlit + Chroma)
docker run --gpus all -d \
  --name rag_app \
  --network rag-net \
  -p 8501:8501 \
  -v rag_data:/app/data \
  -e CHROMA_DIR="/app/data/chroma" \
  -e OLLAMA_URL="http://ollama:11434" \
  -e OLLAMA_MODEL="mistral" \
  rag_medieval_app


Interface disponible sur :
ğŸ‘‰ http://localhost:8501

Utilisation de lâ€™interface

Lâ€™UI propose 4 onglets :

ğŸ“„ PDF

Upload dâ€™un ou plusieurs PDF

Extraction texte + chunking + embeddings + indexation dans Chroma

ğŸ–¼ Images

Upload dâ€™images (png, jpg, jpeg, tif, tiff)

OCR (pytesseract, langues lat+fra) + indexation

ğŸ™ Audio / vidÃ©o

Upload de fichiers mp3, wav, m4a, mp4, mkv

Transcription via Whisper (small) + indexation

â“ Questions

Zone de texte pour la question historique

Le RAG rÃ©cupÃ¨re les extraits pertinents, construit un prompt,
appelle Mistral via Ollama et affiche la rÃ©ponse argumentÃ©e,
avec les sources mobilisÃ©es (liste des fichiers utilisÃ©s).

RÃ©initialiser lâ€™index

Pour repartir avec une base vectorielle vide :

docker stop rag_app || true
docker rm rag_app || true
docker volume rm rag_data || true


Puis relancer lâ€™app (voir section â€œDÃ©marrage rapideâ€).

Logs et debug

Afficher les logs de lâ€™app :

docker logs rag_app


Afficher les logs dâ€™Ollama (appel LLM) :

docker logs ollama
