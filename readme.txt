
# RAG mÃ©diÃ©val local (Docker + GPU + Ollama + ChromaDB + Streamlit)

Ce projet fournit un **moteur RAG local** pensÃ© pour des corpus historiques/mÃ©diÃ©vaux :

- Ingestion de **PDF**, **images** (OCR) et **audio/vidÃ©o** (Whisper)
- Embeddings via **Sentence Transformers** (`multi-qa-mpnet-base-dot-v1`)
- Indexation dans une base vectorielle **ChromaDB** (persistante sur disque)
- GÃ©nÃ©ration des rÃ©ponses via un **LLM local** (Ollama + `mistral`)
- Interface utilisateur en **Streamlit** (4 onglets)

Tout tourne dans un conteneur Docker GPU sous WSL2.

---

## Architecture

DÃ©pÃ´t minimal :

```text
rag_medieval/
â”œâ”€â”€ Dockerfile          # Image Docker (CUDA, PyTorch, Whisper, Chroma, Streamlit...)
â”œâ”€â”€ rag_api.py          # Backend RAG (ingestion, embeddings, Chroma, LLM)
â””â”€â”€ streamlit_gui.py    # Interface Streamlit (PDF / images / audio / questions)

Pipeline RAG

Ingestion

PDF â†’ texte via pypdf

Images â†’ OCR via pytesseract (lat+fra)

Audio/vidÃ©o â†’ transcription via whisper (modÃ¨le small)

Chunking + embeddings

DÃ©coupage en chunks de texte (~1000 caractÃ¨res, overlap 200)

Encodage via SentenceTransformer("multi-qa-mpnet-base-dot-v1")

Indexation

Stockage des embeddings + textes + mÃ©tadonnÃ©es dans ChromaDB

Client persistant : chromadb.PersistentClient(path=CHROMA_DIR)

Collection : historical_rag

Persistance dans un volume Docker rag_data:/app/data

Question / RÃ©ponse

RequÃªte â†’ embedding â†’ recherche top_k dans Chroma

Construction dâ€™un prompt historien critique (avec extraits numÃ©rotÃ©s)

Appel Ã  Ollama (mistral) via HTTP

RÃ©ponse structurÃ©e + rappel des sources utilisÃ©esPrÃ©requis

Windows 10/11 avec WSL2 (Ubuntu)

Docker Desktop configurÃ© avec backend WSL

GPU NVIDIA compatible CUDA + drivers Ã  jour

NVIDIA Container Toolkit installÃ© cÃ´tÃ© WSL (pour --gpus all)



Dans WSL, vÃ©rifier :


nvidia-smi
docker run --rm --gpus all nvidia/cuda:12.2.2-runtime-ubuntu22.04 nvidia-smi

DÃ©marrage rapide

Depuis WSL (Ubuntu) :

cd ~/rag_medieval

# 1. Construire l'image (GPU)
docker build --no-cache -t rag_medieval_app .

# 2. RÃ©seau Docker pour RAG + Ollama
docker network create rag-net || true

# 3. Lancer Ollama (LLM local)
docker run --gpus all -d \
Â  --name ollama \
Â  --network rag-net \
Â  ollama/ollama

# 4. TÃ©lÃ©charger le modÃ¨le Mistral dans Ollama
docker exec -it ollama ollama pull mistral

# 5. Lancer l'application RAG (Streamlit + Chroma)
docker run --gpus all -d \
Â  --name rag_app \
Â  --network rag-net \
Â  -p 8501:8501 \
Â  -v rag_data:/app/data \
Â  -e CHROMA_DIR="/app/data/chroma" \
Â  -e OLLAMA_URL="http://ollama:11434" \
Â  -e OLLAMA_MODEL="mistral" \
Â  rag_medieval_app



Interface disponible sur :
ğŸ‘‰ http://localhost:8501

Utilisation de lâ€™interface

Lâ€™UI propose 4 onglets :

ğŸ“„ PDF

Upload dâ€™un ou plusieurs PDF

Extraction texte + chunking + embeddings + indexation dans Chroma

ğŸ–¼ Images

Upload dâ€™images (png, jpg, jpeg, tif, tiff)

OCR (pytesseract, langues lat+fra) + indexation

ğŸ™ Audio / vidÃ©o

Upload de fichiers mp3, wav, m4a, mp4, mkv

Transcription via Whisper (small) + indexation

â“ Questions

Zone de texte pour la question historique

Le RAG rÃ©cupÃ¨re les extraits pertinents, construit un prompt,
appelle Mistral via Ollama et affiche la rÃ©ponse argumentÃ©e,
avec les sources mobilisÃ©es (liste des fichiers utilisÃ©s).

RÃ©initialiser lâ€™index

Pour repartir avec une base vectorielle vide :

docker stop rag_app || true
docker rm rag_app || true
docker volume rm rag_data || true


Puis relancer lâ€™app (voir section â€œDÃ©marrage rapideâ€).

Logs et debug

Afficher les logs de lâ€™app :

docker logs rag_app


Afficher les logs dâ€™Ollama (appel LLM) :

docker logs ollama



